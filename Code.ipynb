{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import sklearn.feature_extraction as sk\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse data from csv file\n",
    "# [name] is name of file\n",
    "# [hasLabels] is whether or not this csv includes labels (the test data does not)\n",
    "# returns\n",
    "#    list of strings containing tweet texts,\n",
    "#    corresponding list of labels [1,-1] for tweets\n",
    "def parse_file(name, hasLabels = True):\n",
    "    text_list = []\n",
    "    yTr = []\n",
    "\n",
    "    with open(name) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        linecount = 0\n",
    "        for row in csv_reader:\n",
    "            if linecount == 0:\n",
    "                #print(row)\n",
    "                linecount += 1\n",
    "            else:\n",
    "                text = row[1]\n",
    "                text_list.append(text)\n",
    "                #if \"@\" in text:\n",
    "                #    result = re.match(pattern, string)\n",
    "                #    print(\"Yes\")\n",
    "                if hasLabels:\n",
    "                    yTr.append(int(row[17]))\n",
    "                linecount += 1\n",
    "        print(f'Processed {linecount} lines.')\n",
    "    return text_list, yTr\n",
    "\n",
    "# Split lists for train-test-validation splitting\n",
    "# splits lists [X] and [Y] into two separate shuffled lists each\n",
    "# first set contains [frac] proportion of the elements\n",
    "# returns: firstX, firstY, secondX, secondY\n",
    "def split_data(frac, X, Y):\n",
    "    n = len(X)\n",
    "    sort = np.random.permutation(n)\n",
    "    split = int(frac * n)\n",
    "    Xsort = np.array(X)[sort]\n",
    "    Ysort = np.array(Y)[sort]\n",
    "    return Xsort[0:split], Ysort[0:split], Xsort[split:], Ysort[split:]\n",
    "\n",
    "# Create submission csv with file name [name] for predicted values [predicted]\n",
    "def write_submission(name, predicted):\n",
    "    with open(name, mode='w') as submission:\n",
    "        sub_writer = csv.writer(submission, delimiter=',', lineterminator='\\n')\n",
    "        sub_writer.writerow(['ID', 'Label'])\n",
    "\n",
    "        for i in range(len(predicted)):\n",
    "            sub_writer.writerow([i,predicted[i]])\n",
    "\n",
    "# For pre-processing on the data set, element-wise modify each text string\n",
    "def modify_text(text_list, yTr, mod_func):\n",
    "    new_list = []\n",
    "    for i in range(len(text_list)):\n",
    "        new_list.append(mod_func(text_list[i], yTr[i]))\n",
    "    return new_list\n",
    "\n",
    "http_pattern = r\".*http[^ \\t\\n\\r\\f\\v]+[ \\t\\n\\r\\f\\v]*\"\n",
    "\n",
    "# prune out http entirely\n",
    "def prune_http(text_list, yTr):\n",
    "    return modify_text(text_list, yTr, lambda s, y: re.sub(http_pattern, '', s))\n",
    "\n",
    "# replace http with a special word\n",
    "def replace_all_http(text_list, yTr):\n",
    "    return modify_text(text_list, yTr, lambda s, y: re.sub(http_pattern, 'IS_EMBED', s))\n",
    "\n",
    "# final decision on how to pre-process the data, replaced only the final hash chars of the shortened link\n",
    "# https://t.co/... -> https://t.co/\n",
    "def pre_process_text(text_list):\n",
    "    return modify_text(train_text, yTr, lambda s, y: re.sub(r\"https://t.co/\\S+\", 'https://t.co/', s))\n",
    "\n",
    "# Test random forest on pre-computed training and testing data\n",
    "# [trees] is th enumber of estimators\n",
    "# [M] is the number of trials to perform\n",
    "# [progress_output] determines whether or not to print output per trial\n",
    "def test_random_forest(xTr_counts, yTr, xTe_counts, yTe, trees=500, M=1, progress_output = True):\n",
    "    if yTe is None:\n",
    "        print(\"Cannot test accuracy – no labels for testing data\")\n",
    "        return\n",
    "    avg_acc = 0\n",
    "    accuracies = []\n",
    "    num_predictions = len(yTe)\n",
    "    for i in range(M):\n",
    "        H = RFC(n_estimators=trees).fit(xTr_counts, yTr)\n",
    "        predicted = H.predict(xTe_counts)\n",
    "        acc = np.sum(predicted == yTe)/num_predictions\n",
    "        if progress_output:\n",
    "            print(\"Trial {}: {}\".format(i+1, acc))\n",
    "        accuracies.append(acc)\n",
    "    print(\"Mean: {}, Std.dev: {}\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "    \n",
    "# Test random forest on random splits of the training data\n",
    "# [trees] is th enumber of estimators\n",
    "# [M] is the number of trials to perform\n",
    "# [progress_output] determines whether or not to print output per trial\n",
    "# [split] is the proportion of training vs testing\n",
    "def test_random_forest_full(trees=500, M=1, progress_output = True, split = 0.7):\n",
    "    train_text, yTr = parse_file(\"train.csv\")\n",
    "    train_text = modify_text(train_text, yTr, lambda s, y: re.sub(r\"https://t.co/\\S+\", 'https://t.co/', s))\n",
    "    avg_acc = 0\n",
    "    accuracies = []\n",
    "    num_predictions = int((1 - split) * len(yTr))\n",
    "    for i in range(M):\n",
    "        train_text0, yTr0, test_text, yTe = split_data(split, train_text, yTr)\n",
    "        count_vec = sk.text.TfidfVectorizer(analyzer=\"char\",strip_accents=\"unicode\", lowercase=False)#, token_pattern=r'\\w{1,}')\n",
    "        xTr_counts = count_vec.fit_transform(train_text0)\n",
    "        xTe_counts = count_vec.transform(test_text)\n",
    "        H = RFC(n_estimators=trees).fit(xTr_counts, yTr0)\n",
    "        predicted = H.predict(xTe_counts)\n",
    "        acc = np.sum(predicted == yTe)/num_predictions\n",
    "        if progress_output:\n",
    "            print(\"Trial {}: {}\".format(i+1, acc))\n",
    "        accuracies.append(acc)\n",
    "    print(\"Mean: {}, Std.dev: {}\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "    #return accuracies\n",
    "\n",
    "# A classifier that produces predictions solely based on whether or not the tweet text contains a link\n",
    "# our baseline that our true predictor must surpass\n",
    "def cheater_classifier(text_list):\n",
    "    predictions = []\n",
    "    for i in range(len(text_list)):\n",
    "        #print(text_list[i], \"|\", re.match(http_pattern, text_list[i]))\n",
    "        predictions.append(-1 if re.match(http_pattern, text_list[i]) else 1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1090 lines.\n",
      "Processed 301 lines.\n"
     ]
    }
   ],
   "source": [
    "# train on entire set, test on submission csv\n",
    "train_text, yTr = parse_file('train.csv')\n",
    "test_text, yTe = parse_file('test.csv', hasLabels = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1090 lines.\n"
     ]
    }
   ],
   "source": [
    "# train on 70% of set, test on 30%\n",
    "train_text, yTr = parse_file(\"train.csv\")\n",
    "train_text, yTr, test_text, yTe = split_data(0.7, train_text, yTr)\n",
    "#print(np.sum(cheater_classifier(test_text) == yTe)/300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327, 84)\n",
      "[(' ', 382.92226057491297), ('e', 199.4582485668008), ('t', 176.0743631154441), ('a', 159.2662491907815), ('o', 155.48287787719113), ('n', 130.6373422071858), ('i', 128.01291619805), ('r', 127.1384183017574), ('s', 103.19368654095491), ('l', 96.56467781008011), ('/', 89.06366513431713), ('h', 84.78415106861591), ('d', 71.06749671783851), ('u', 67.4725027637438), ('p', 65.06493621287572), ('m', 62.545479531046794), ('c', 59.88126541328635), ('y', 56.36582603320831), ('g', 47.237177400515826), ('T', 46.89403514374512), ('w', 45.733235424038156), ('A', 43.6955634292396), ('.', 43.11776026166888), ('b', 37.458477956567314), ('f', 36.856246277612826), ('k', 36.61314656983876), (':', 35.84519755138385), ('#', 34.99448656248796), ('!', 32.47774603272562), ('@', 31.912694490697298), ('0', 28.592668770276216), ('I', 28.40146048712822), ('v', 27.63533756769912), (';', 27.50952537816148), ('M', 26.220559314938992), ('C', 26.07269142779909), ('N', 23.952150397478995), ('S', 23.423369830474453), ('E', 22.950640596474383), ('1', 22.74286901079109), ('G', 21.794018057803314), ('\"', 21.768657137823773), ('2', 20.824865312283865), ('R', 20.33218094570768), ('6', 19.18859913516113), ('D', 18.735084883202422), ('W', 18.43408903139008), ('O', 17.451367623618864), ('H', 17.31993502804624), ('P', 17.18379910112953), ('-', 16.374668546670545), ('F', 14.945732755958673), ('B', 13.229543255545876), ('j', 12.957296736648328), (\"'\", 12.341979037810273), ('V', 12.075981876467159), ('J', 12.012378692264438), ('L', 11.667708624735203), ('z', 11.400667291803314), ('Y', 10.88651588720351), ('x', 10.620094893247156), ('U', 9.739379676374032), ('K', 8.672729497294215), ('7', 6.555401082336581), ('3', 5.989559802062656), ('?', 5.523424431168742), ('5', 5.326022734466199), ('4', 4.835763320030737), ('8', 4.724749583780933), ('&', 4.371606238597732), ('<', 4.072846872589899), ('>', 4.072846872589899), ('_', 3.9679748414741067), ('(', 3.5036956957939602), (')', 3.5036956957939602), ('q', 3.362265127523281), ('%', 3.2797462026258772), ('9', 3.021081295382626), ('+', 2.9504843673288845), ('$', 2.165995113731898), ('Z', 2.063228707284349), ('X', 1.341136966372228), ('Q', 0.8356023347969236), ('=', 0.33333381534904694)]\n"
     ]
    }
   ],
   "source": [
    "# Create bag of words for training set\n",
    "\n",
    "train_text0 = pre_process_text(train_text)\n",
    "#train_text0 = replace_all_http(train_text0, yTr)\n",
    "#train_text0 = modify_text(train_text0, yTr, lambda s, y: re.sub(r\"https://t.co/\\S+\", 'https://t.co/', s))\n",
    "#train_text0 = modify_text(train_text0, yTr, lambda s, y: re.sub(r\"<\\S+>\", '∑', s))\n",
    "#train_text0 = modify_text(train_text0, yTr, lambda s, y: re.sub(r' ', '', s))\n",
    "\n",
    "#count_vec = sk.text.CountVectorizer(analyzer=\"char\",strip_accents=\"unicode\", lowercase=False)\n",
    "count_vec = sk.text.TfidfVectorizer(analyzer=\"char\",strip_accents=\"unicode\", lowercase=False)#, token_pattern=r'\\w{1,}')\n",
    "xTr_counts = count_vec.fit_transform(train_text0)  # feature vector\n",
    "xTr_counts.shape\n",
    "\n",
    "sum_words = xTr_counts.sum(axis=0)\n",
    "\n",
    "word_freq = [(word, sum_words[0,idx]) for (word,idx) in count_vec.vocabulary_.items()]\n",
    "word_freq = sorted(word_freq, key=lambda x:x[1], reverse=True)  # count of each word overall\n",
    "\n",
    "# Create bag of words for testing set\n",
    "xTe_counts = count_vec.transform(test_text)  # feature vector\n",
    "\n",
    "print(xTe_counts.shape)\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: 0.8654434250764526\n",
      "Trial 2: 0.8685015290519877\n",
      "Trial 3: 0.8715596330275229\n",
      "Trial 4: 0.8654434250764526\n",
      "Trial 5: 0.8715596330275229\n",
      "Trial 6: 0.8685015290519877\n",
      "Trial 7: 0.8685015290519877\n",
      "Trial 8: 0.8685015290519877\n",
      "Trial 9: 0.8654434250764526\n",
      "Trial 10: 0.8685015290519877\n",
      "Mean: 0.8681957186544341, Std.dev: 0.0021406727828746307\n"
     ]
    }
   ],
   "source": [
    "test_random_forest(xTr_counts, yTr, xTe_counts, yTe, trees=1000, M=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_submission('link-only-submission.csv', cheater_classifier(test_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
